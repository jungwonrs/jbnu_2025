import torch
import os
from transformers import (
    Blip2ForConditionalGeneration, 
    Blip2Processor,
    FuyuForCausalLM, 
    FuyuProcessor,
    LlavaForConditionalGeneration, 
    AutoProcessor,
    CLIPModel,
    CLIPProcessor,
    BitsAndBytesConfig
)

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# [RTX 5080 16GB ìµœì í™”] 4ë¹„íŠ¸ ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# --- [ë¡œê·¸ í—¬í¼ í•¨ìˆ˜] ---
def smart_load(class_obj, model_id, **kwargs):
    print(f"   ğŸ” '{model_id}' ì°¾ëŠ” ì¤‘...", end=" ")
    try:
        obj = class_obj.from_pretrained(model_id, local_files_only=True, **kwargs)
        print("âœ… [Cache] ë¡œì»¬ ë°œê²¬!")
        return obj
    except Exception:
        print("ğŸŒ [Download] ìºì‹œì— ì—†ìŒ. ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        obj = class_obj.from_pretrained(model_id, local_files_only=False, **kwargs)
        print("      -> ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")
        return obj

def load_blip2_base():
    print("\n--- BLIP-2 (Base) 4-bit ---")
    model_id = "Salesforce/blip2-opt-2.7b"
    
    model = smart_load(
        Blip2ForConditionalGeneration, 
        model_id, 
        quantization_config=bnb_config, 
        device_map="auto"
    )
    processor = smart_load(Blip2Processor, model_id)
    return model, processor

def load_fuyu():
    print("\n--- Fuyu-8B 4-bit ---")
    model_id = "adept/fuyu-8b"
    
    print(f"   ğŸ” '{model_id}' (4-bit) ì‹œë„...", end=" ")
    try:
        model = FuyuForCausalLM.from_pretrained(
            model_id, quantization_config=bnb_config, device_map="auto", local_files_only=True
        )
        print("âœ… [Cache] ì„±ê³µ")
    except Exception:
        print("âš ï¸ [Cache Miss or Error] ì‹¤íŒ¨. ì¸í„°ë„· ë‹¤ìš´ë¡œë“œ ë˜ëŠ” FP16 ì‹œë„...")
        try:
             model = FuyuForCausalLM.from_pretrained(
                model_id, quantization_config=bnb_config, device_map="auto", local_files_only=False
            )
        except:
            print("      -> 4-bit ì‹¤íŒ¨, FP16ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ...")
            model = FuyuForCausalLM.from_pretrained(
                model_id, torch_dtype=torch.float16, device_map="auto"
            )
            
    processor = smart_load(FuyuProcessor, model_id)
    return model, processor

def load_llava13():
    print("\n--- LLaVA-13B 4-bit ---")
    model_id = "llava-hf/llava-1.5-13b-hf"
    
    model = smart_load(
        LlavaForConditionalGeneration, 
        model_id, 
        quantization_config=bnb_config, 
        device_map="auto"
    )
    processor = smart_load(AutoProcessor, model_id)
    return model, processor

def load_clip_vit_l():
    print("\n--- CLIP-Large ---")
    model_id = "openai/clip-vit-large-patch14"
    
    print(f"   ğŸ” '{model_id}' ì°¾ëŠ” ì¤‘...", end=" ")
    try:
        model = CLIPModel.from_pretrained(model_id, local_files_only=True).to(DEVICE)
        print("âœ… [Cache] ì„±ê³µ")
    except:
        print("ğŸŒ [Download] ë‹¤ìš´ë¡œë“œ...")
        model = CLIPModel.from_pretrained(model_id, local_files_only=False).to(DEVICE)
        
    processor = smart_load(CLIPProcessor, model_id)
    return model, processor