import torch
import os
from transformers import (
    AutoProcessor, 
    LlavaForConditionalGeneration, 
    InstructBlipForConditionalGeneration, 
    InstructBlipProcessor,
    CLIPModel,
    CLIPProcessor,
    ViTForImageClassification,
    ViTImageProcessor,
    BitsAndBytesConfig
)
from torchvision.models import resnet50, ResNet50_Weights, vgg16, VGG16_Weights

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# [RTX 5080 16GB ìµœì í™”] 4ë¹„íŠ¸ ë¡œë”© ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

# --- [ë¡œê·¸ í—¬í¼ í•¨ìˆ˜] ---
def smart_load(class_obj, model_id, **kwargs):
    """
    1. ë¡œì»¬ ìºì‹œ(local_files_only=True)ë¡œ ë¨¼ì € ì‹œë„
    2. ì‹¤íŒ¨í•˜ë©´ ì¸í„°ë„·(local_files_only=False)ìœ¼ë¡œ ì‹œë„
    í•˜ë©° ë¡œê·¸ë¥¼ ì¶œë ¥í•¨.
    """
    print(f"   ğŸ” '{model_id}' ì°¾ëŠ” ì¤‘...", end=" ")
    try:
        # 1. ìºì‹œ ë¡œë“œ ì‹œë„
        obj = class_obj.from_pretrained(model_id, local_files_only=True, **kwargs)
        print("âœ… [Cache] ë¡œì»¬ ë°œê²¬! (ì¸í„°ë„· X)")
        return obj
    except Exception:
        # 2. ì‹¤íŒ¨ ì‹œ ë‹¤ìš´ë¡œë“œ
        print("ğŸŒ [Download] ìºì‹œì— ì—†ìŒ. ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
        obj = class_obj.from_pretrained(model_id, local_files_only=False, **kwargs)
        print("      -> ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ ì™„ë£Œ.")
        return obj

class vml_model:
    @staticmethod
    def llava7():
        print("\n--- LLaVA-1.5-7B (4-bit) ---")
        model_id = "llava-hf/llava-1.5-7b-hf"
        
        model = smart_load(
            LlavaForConditionalGeneration, 
            model_id, 
            quantization_config=bnb_config, 
            device_map="auto"
        )
        processor = smart_load(AutoProcessor, model_id)
        return model, processor

    @staticmethod
    def instructblip():
        print("\n--- InstructBLIP (4-bit) ---")
        model_id = "Salesforce/instructblip-flan-t5-xl"
        
        model = smart_load(
            InstructBlipForConditionalGeneration, 
            model_id, 
            quantization_config=bnb_config, 
            device_map="auto"
        )
        processor = smart_load(InstructBlipProcessor, model_id)
        return model, processor

    @staticmethod
    def clip_vit_b():
        print("\n--- CLIP (ViT-B/32) ---")
        model_id = "openai/clip-vit-base-patch32"
        
        # CLIPì€ ëª¨ë¸ í¬ê¸°ê°€ ì‘ìœ¼ë¯€ë¡œ 4ë¹„íŠ¸ ì—†ì´ ë°”ë¡œ GPUë¡œ ë¡œë“œ
        print(f"   ğŸ” '{model_id}' (No-Quant) ì°¾ëŠ” ì¤‘...", end=" ")
        try:
            model = CLIPModel.from_pretrained(model_id, local_files_only=True).to(DEVICE)
            print("âœ… [Cache] ì„±ê³µ")
        except:
            print("ğŸŒ [Download] ë‹¤ìš´ë¡œë“œ...")
            model = CLIPModel.from_pretrained(model_id, local_files_only=False).to(DEVICE)
            
        processor = smart_load(CLIPProcessor, model_id)
        return model, processor


class classifier_model:
    @staticmethod
    def vit():
        print("\n--- ViT-B/32 ---")
        model_id = "google/vit-base-patch16-224"
        
        print(f"   ğŸ” '{model_id}' ì°¾ëŠ” ì¤‘...", end=" ")
        try:
            model = ViTForImageClassification.from_pretrained(model_id, local_files_only=True).to(DEVICE)
            print("âœ… [Cache] ì„±ê³µ")
        except:
            print("ğŸŒ [Download] ë‹¤ìš´ë¡œë“œ...")
            model = ViTForImageClassification.from_pretrained(model_id, local_files_only=False).to(DEVICE)
            
        processor = smart_load(ViTImageProcessor, model_id)
        return model, processor

    @staticmethod
    def resnet():
        print("\n--- ResNet-50 ---")
        print("   âœ… [Built-in] Torchvision ë‚´ì¥ ëª¨ë¸ ì‚¬ìš©")
        weights = ResNet50_Weights.IMAGENET1K_V1
        model = resnet50(weights=weights).to(DEVICE)
        model.eval()
        processor = weights.transforms()
        return model, processor

    @staticmethod
    def vgg():
        print("\n--- VGG16 ---")
        print("   âœ… [Built-in] Torchvision ë‚´ì¥ ëª¨ë¸ ì‚¬ìš©")
        weights = VGG16_Weights.IMAGENET1K_V1
        model = vgg16(weights=weights).to(DEVICE)
        model.eval()
        processor = weights.transforms()
        return model, processor